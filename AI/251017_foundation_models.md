# Image Foundation Model

- 강사 : KAIST 오태현 교수



## Keywords

- Foundation model
- VLM
- CLIP



### 파운데이션 모델의 활용

- 이미 소프트/하드웨어 인프라가 transformer에 맞춰져 있어서, 다른 메커니즘이 발명되더라도 한동안 주요 모델들은 transformer 중심이 될 가능성이 매우 높다!
- foundation 모델을 뭘 쓰느냐에 따라 성능 차이가 극단적으로 갈리는 케이스는 별로 없다.
- 일반적으로 데이터를 어떻게 정제하느냐가 훨씬 중요하다.



## 질문 / 답변

- Q1. 내가 빵 객체 인식해서 종류를 구분하는 object detection 모델을 만든다고 쳐봐. foundation model을 yolo로 해서 전이학습을 시킨다고 했을 때, yolo에서 빵이랑 관련되지 않은 학습내용들이 대부분일텐데 걔네도 기여를 하는거야? 아니면 노이즈만 되는거야?

  - 사전학습 내용 중 빵과 관련 없는 내용들도 시각적 일반 지능을 형성하는데 기여함.

    - 빛이 물체에 닿을 때 생기는 하이라이트
    - 그림자와 질감의 관계
    - 물체가 배경과 어떻게 분리되는가
    - 카메라 거리와 크기의 상관관계 같은 것들

  - 그러나 class를 직접 추측하는 layer에는 빵과 상관 없는 클래스들에 대한 내용들이 많아서 노이즈를 만들기도 함.

  - 그래서 일반적인 시각 특징을 학습하는 backbone쪽은 대부분 freeze하고, 클래스를 예측하는 Head 부분은 새로 학습하는 방식을 씀.

    - 그리고 전이학습할 데이터의 종류와 양이 많을 수록 unfreeze하는 영역이 backbone 일부까지 더 넓어지게 됨.

    

- Q2. 지도학습할 때 라벨을 주잖아. 거기에 transformer 같은 언어 관련 모델을 함께 넣으면 어떨까 하는 아이디어가 떠올랐는데 어때? 예를 들면 '깨찰빵'이라면 그 라벨에는 깨찰빵을 나타내는 이미지 뿐만아니라 '깨, '찰', '빵'이라는 subword들에 대한 의미가 같이 담기고, 그 subword들이 일반적으로 가지고 있는 이미지까지도 복합적으로 학습 및 추론에 활용하는 거지. 비슷한 아이디어가 이미 있었겠지?

  - 그런 방식이 오늘 배울 VLM의 핵심 개념이야. CLIP 같은 모델에서 나타나는 패턴.
  - 이미지와 텍스트를 함께 학습시키고 시각적 특징과 언어적 의미를 공통 임베딩 공간에서 연결하는 방식.
  - 근데 정확하게는 subword라기보단 token이라고 봐야지. tokenizing 방식에 따라 좀 다르겠지만 각 token이 subword랑 같아지는 경우가 많긴 하지만.



- Q3. 여러 개의 subword들로 이루어진 단어는 이미지에 언어정보가 결합되면 모델 성능이 올라갈 수 있을 것 같은데, 한 글자짜리 한자어처럼 더이상 의미적으로 쪼갤 수 없는 단어는 벡터에 언어적 정보를 추가해봤자 노이즈만 되는 거 아닐까? 가령 주어진 이미지가 코뿔소라면 그 이미지를 추론하는데에 '코', '뿔', '소'가 다 도움이 되지만, '용'이라면 언어벡터에는 그냥 '용'은 '용'이라는 정보밖에 없는 거잖아.
  - 애초에 트랜스포머가 학습하고 추론하는 방식이 특정 토큰을 일대일로 대응시키는게 아니라 주변의 다른 토큰들과의 관계와 맥락이 학습되는 형태이기 때문에 더 쪼갤 수 없는 단어라고 해도 충분히 의미를 가질 수 있음. '용'에는 '용'이 담기는게 아니라 '용'이라는 토큰이 사용되는 통계적 맥락(신화, 전설, 수염, 비늘...)이 담겨 있지.
  - 물론 코뿔소처럼 subword의 내부 조합을 통해 학습되는 의미는 없겠지만, 언어 모델이 만들어내는 정보는 그게 다가 아니고, 그 단어 외부의 문맥을 통해 학습된 의미도 있기 때문에 VLM이 이미지를 이해하는 데 도움을 줄 수 있는 거지.
  - 물론 이건 VLM에서만 그런게 아니라 transformer 기반 언어 모델의 고유한 특징임. 잘 이해하기.