# CV

- 강사 : KAIST 심현정 교수



## Keywords



## 질문 / 답변



- Q1. RGB형식으로 이미지를 저장할 때, 각 픽셀마다 3차원 채널(R,G,B)의 값이 들어가야 하는데, 회색조 픽셀은 1채널(0~255), 흑백 픽셀은 1채널 이진(0,1) 값만 들어가도 되니까 회색조나 흑백으로 표현할 수 있는 픽셀에 대해서만 그런 방식으로 저장하면 더 메모리효율적이지 않을까?
    - 맞음. TIFF, PDF등의 포맷에서 그러한 방식을 사용하고 있음. 다만 그 픽셀을 RGB값을 가진 값으로 수정해야 할 경우 매번 RGB 형식으로 디코딩해야 하고, 표준화가 어려워 호환성이 떨어질 수 있기 때문에 특수한 목적이 있을 때만 사용하는 방법임.
    - 메모리 효율성이 매우 중요하고 수정, 표준화 요구가 없는 환경에서는 사용하면 좋음. 언리얼엔진이나 유니티 같은 데서도 자주 사용하는 최적화 방식 중 하나임

- Q2-1. 왜 회색조 이미지도 0~255의 값을 사용했을까? 어차피 회색조면 좀 더 적은 분류로도 충분할 것 같기도 한데
    - 1비트(8바이트)가 기본 데이터 단위라 메모리 효율이 극대화됨
    
    - 사람이 감지할 수 있는 밝기 차이가 2~300단계정도 돼서 딱 알맞음
    
    - 7비트(128단계)로 줄여봐야 12.5%정도밖에 절약 안 됨
    
    - 옛날에 리소스가 부족할 때는 2비트나 4비트 등 훨씬 더 적은 분류로도 쓰긴 했음. 그 극단이 이진화된 흑백 이미지인거고
    
    - 지금도 최적화가 정말 중요하면 7비트나 그 이하로도 물론 사용할 수는 있음.  다만 여러 확장자나 프로그램와의 호환성 등이 문제가 될 수 있음
    
      
    
- Q2-2. 그럼 흑백 이진분류만 사용하면서, 픽셀별로 밝기만 다르게 만들어서 출력하는 방법은 없나? 그게 더 효율적일 수도 있지 않을까?
    - 픽셀별로 밝기를 다르게 하면 그게 회색조 이미지가 되는 거지.
    - 흑백으로 표현하는 방법은 Dithering이라고 있는데, 픽셀별 실제 밝기값을 다르게 하는게 아니라, 흑색 픽셀 3개 + 백색 픽셀 1개면 밝기 25% 이런 식으로 밝기감을 표현하는 방식은 있음. 흑백프린터나 초기 디스플레이에 활용되었던 기술이고 메모리 사용량이 매우 적음.
    - 물론 정교한 표현에 있어서는 한계가 명확하지만 현대에도 특정 상황에서는 응용되고 있음



- Q3-1. 이미지 증강할 때 RGB값의 패턴은 유지하면서 값을 조금씩만 바꾸는 방식도 있지 않을까?
    - 있음. 약간의 난수(가우시안 노이즈 등) 추가, 밝기/대비 등을 조금씩만 조정하는 방식을 Jittering이라고 함.
    - 그 외에도 Mixup, random erasing, 스타일 변환, 랜덤 변환 등 다양한 증강법이 있고 상황에 맞게 사용하면 됨.
- Q3-2. 근데 가우시안 노이즈가 실제 촬영시 발생하는 노이즈랑은 꽤 다를 수도 있지 않나? 노이즈가 렌즈에 뭐 묻어서 특정 영역에만 어떤 패턴을 가지고 나타난다든지, 여러 환경적 요인에 따라 수집할 데이터의 노이즈의 특성이 다 다를텐데?
    - 가우시안 노이즈로 충분한 경우도 있고 아닌 경우도 있고.. 특정 환경에서 추출한 데이터들의 노이즈 패턴을 학습시켜서 그러한 노이즈를 그대로 입히는 것도 가능함.
    
    

- Q4-1. 명도, 채도, 대비 같은 값들은 RGB값과 별개의 채널에 저장되는 거야?
    - ㄴㄴ. 명도 채도 대비는 RGB와 별개로 있는게 아니라, 현재 RGB값에 대하여 상대적으로만 존재하는 값임
- Q4-2. 그럼 RGB값과 명도 채도 대비를 주면 새로운 RGB값을 반환할 수 있는 거야?
    - ㅇㅇ. RGB → HSV → RGB로 채널을 변환하여 새로운 RGB값을 반환하는 방식이 주로 사용됨.
- Q4-3. 근데 프로그램에서 원래의 색을 복원하거나 명도 채도 대비를 재조정하려면 매번 새로운 값만 새로 할당하는게 아니라 원본 RGB값이 남아있어야 할 거 아냐
    - 물론 원래 색상에 대한 정보는 따로 저장해놔야 함. 그렇지 않으면 흰색(채도 0)이 된다든지 했을 때 복원할 방법이 없겠지. 기존 색상을 저장해놓으면 그만큼 많은 리소스를 잡아먹는 대신 복원성이 높을 거고, 명도 채도 대비만 조정해가며 바로바로 반환하면 연산량은 줄어드는 대신 할 수 없는 작업도 생기겠지.



- Q5-1. NMS에서 confidence score가 어떻게 구해지는 거? 정답 라벨과의 IOU값인가?
    - ㄴㄴ. NMS 추론 과정에서는 정답 라벨값을 사용하지 않음. 모델이 클래스의 목록은 알고 있지만 정답 클래스는 모름.
    - confidence score는 해당 바운딩 박스 안에 객체가 존재할 확률 x 그 객체가 특정 클래스일 조건부확률임.  객체가 있을 만한 영역을 찾는 일과 객체의 클래스를 맞히는 일은 별개로 동작함.
- Q5-2. 뭔소리야 정답 라벨값이 있는데 왜 그걸 써서 학습하지 않아?
    - 학습은 나중에 정답 라벨값을 보고서 하지. 근데 NMS는 학습을 시키는 과정이 아니라, 추론을 할 때 여러 개의 예측값 중에 제일 그럴듯한 놈들만 남겨놓기 위한 알고리즘이라고. 학습이랑은 상관이 없음.
- Q5-3. 그렇게 추론 단계에서 비슷한 위치에 있는 많은 바운딩박스들을 없애버리고 가면 학습시 정답 라벨과 비교할 때 위치 차이가 커질 수도 있잖아. 순서를 바꿔서 학습시에 비교해서 탈락시키기엔 리소스가 너무 많이 들어서 이런 방식으로 하는 건가?
    - 딱 그거 맞음. NMS는 추론 과정을 경량화하는 알고리즘이고 학습시에는 이용되지 않는다. 이렇게 이해하고 넘어가면 맞을 듯.



- Q6. MobileNetV1은 MobileNetV2에 비해 성능도 낮고 파라미터도 더 많이 쓰는 완전한 하위호환으로 보이는데, 이런 모델들도 계속 라이브러리에서 삭제하지 않고 남겨두는 이유가 뭐야?

  - 기존에 그 모델을 기반으로 만든 프로그램들의 호환성과 재현성을 유지하기 위함.

  - 더 단순한 구조를 가지고 있다든지 하는 이유로 이전 버전의 모델이 초저전력 환경에서 더 적합한 경우도 있음.



- Q7-1. 커널이 2차원 특성 맵을 만든다는게 무슨 소리지?
    - 이미지로 치면, n*n(픽셀) 크기의 커널이 이미지 전체를 순회하면서 (x, y, (r, g, b))의 shape을 띠는 새로운 배열을 만들어내고, 그걸 학습한다는 거야. 2차원 특성 맵을 여러 개 만들고, 걔네를 합쳐서 실제로는 3차원 배열을 만들어 학습에 이용하는 거지.
- Q7-2. 근데 x와 y의 자리에도 단일 위치값이 아니라 범위값이 들어가야 하는 거 아닌가?
    - 물론 맞음. x와 y 자리에는 공간값 내지는 범위값이 들어가는 거고, 데이터의 형태에 따라 (r, g, b) 자리에도 더 단순하거나 복잡한 형태의 정보가 얼마든지 담길 수 있음.
- Q7-3. 회색조나 흑백이미지라면 물론 그만큼 저차원의 정보가 3차원 자리에 담길 거고.. 이미지 외에 다른 데이터에 대해서도 CNN을 쓰나?
    - ㅇㅇ. 처음엔 이미지 분석용으로 나왔지만, 공간성이나 시계열성을 갖는 많은 데이터에 대해서는 모두 활용할 수 있음. 텍스트, 오디오, 이미지, 비디오 등 뭐든 가능



- Q8-1. VIT가 transformer를 이미지 처리에 활용한 모델이라고 하는데.. transformer 자체가 대단히 좋은 메커니즘이긴 한데 그게 근본적으로 이미지 처리랑 어울리는 방식인지는 잘 모르겠네. 되는게 신기하다.
  - 그 생각이 맞긴 함
  - 이미지는 가까운 픽셀들 사이에 강한 상관관계를 갖고, CNN은 커널이 인접 픽셀들을 묶어서 처리하는데, transformer는 멀리 떨어진 픽셀과의 관계도 전역적으로 계산해버림
  - 그래서 원형 메커니즘 자체는 그닥 어울리지 않는 것도 맞는데, 결국 돌려봤을 때 결과가 괜찮음
  - 정확하게는, 초기학습 성능은 CNN보다 훨씬 떨어지지만 데이터셋이 수억 장 규모로 충분히 많아졌을 때는 CNN보다 더 강해지는 경향이 있음
  - 그리고 구조가 거의 같으니까 NLP와 CV를 통합하기에도 좋고..
  - 세부 아키텍쳐도 좀 조정해볼 수 있고, 꼭 CNN과 VIT 중 하나를 선택해야만 하는 것도 아니고.. 어떻게 튜닝하고 조합해서 사용할 것인지에 대해 연구가 이루어지고 있는 거지.
- Q8-2. transformer가 픽셀간 위치관계를 반영하지 못하는게 문제라면, VIT에 픽셀의 위치정보를 넣어서 가까운 위치관계에 대해 더 강한 가중치를 부여하려는 시도가 당연히 있었을 것 같은데?
  - 물론 그렇지. 당연히 다들 그런 것부터 시도했고 실제로 성과도 있음.
    - 처음에는 그냥 절대적 위치 벡터를 더해주는 단순한 로직이었지만
    - 벡터값에 상대적인 위치정보를 넣어준다거나
    - 인접한 patch들끼리만 attention을 수행하는 CNN과 매우 비슷한 방식을 써보기도 하고
    - x,y좌표를 따로 나눠서 위치정보를 2차원으로 넣어주기도 하고..
    - 이미지 한장을 여러 window로 나눠서 그 안에서만 attention을 수행하고 슬라이딩하기도 함
  - 결국 transformer의 전역적 표현력과 CNN의 지역적 구조 인식을 잘 조합한 경우가 가장 성능이 좋다는게 검증됨.



- Q9. leaky relu가 relu의 개선된 버전처럼 보이는데 왜 여전히 relu가 더 범용적으로 많이 쓰이는 거야?
  - 그냥 경험적으로 성능이 딱히 차이가 없거나 relu가 충분히 좋아서 그래.
    - 현대 학습 기법들을 적용하면 relu가 음수 input 발생 자체를 대부분 막아주기도 하고
    - 음수 기울기 남겨서 괜히 그게 노이즈처럼 작용해서 학습이 불안정해지는 경우도 있고..
      - 음수기울기 가중치인 alpha값은 보통 0.01을 쓰고 튜닝이 가능한데
      - 그 수치도 이론적 기반이 있는게 아니라 그냥 경험적인 거라 좀 애매함
    - 구현이 살짝 복잡해지니까 속도 자체도 떨어지고.. 그래서 기본적으로는 잘 안 씀
  - 실질적으로 너네가 활성화함수 선택할 때는..
    - 그냥 일단 relu(+He 초기화) 박고 시작하면 됨.
    - relu로 돌려봤을 때 값을 보고 leaky relu나 다른 주요 활성화함수들 고려하는 거.
    - Sigmoid나 Tanh같은 건 연구자 급이 세부적으로 튜닝해서 쓰는게 아니면 쓸 일 없다고 봐도 됨

