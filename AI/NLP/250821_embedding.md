# 임베딩

- 신한 해커톤 bmpm 프로젝트 진행중
  - RAG-based LLM 구현
  - https://github.com/1227wano/Bapsim/tree/dev/AI
    - `AI/rag_data/make_embedding.py`에서 e5계열 모델 활용하여 임베딩 생성
    - `AI/app/main.py`에서 faiss로 임베딩 로드하여 context로 llm에 전달

## 개념

- 텍스트(또는 이미지 등)를 고정 길이의 **실수 벡터**로 바꾼 것. 의미가 비슷한 것끼리 가까운 공간에 모이도록 학습됨.
- 단순 키워드 매칭으로는 놓치는 의미적 유사성을 잡기 위해 사용
- 활용처
  - **RAG**에서 관련 문서 후보 찾는 1차 필터 역할
  - **추천 시스템** 등에서 의미 기반 검색에 활용
  - 그 외에도 분류, 클러스터링 등 여러 작업에서 활용 가능
- E5, BGE, GTE, MPNet 등의 모델이 있음
  - 같은 계열 모델 내에서도 차원수 `base(384/768)`, `large(1024)` 에 따라 분화됨
  - 차원수가 높을수록 품질도 대체로 좋아지지만 메모리 사용량이 많아짐
- 코사인 유사도 또는 L2 거리 기반으로 벡터 생성



## 유사도 | 거리 개념

- **IP / Cosine similarity**: 두 벡터의 각도. L2 정규화 후 **내적(Inner Product)**
- **L2 거리**: 두 점 사이의 유클리디안 거리. 모델/인덱스 설정에 맞춰 IP 또는 L2를 일관되게 사용.



## 청킹과 전처리

- 긴 문서를 통으로 임베딩하면 내용간 세부적 매칭이 약해 문맥을 잃어버림
- 문맥을 잃지 않도록 적당 길이한 길이로 자르고 겹치는 부분(overlap)을 만듦
- 일반적으로 사용하는 공정
  - 길이: 500~1000자 내외
  - 겹침: 10~20% (답변에 꼭 필요한 문맥 보존)
  - 최소 길이 필터: 너무 짧은 조각에서 오는 노이즈 줄이기 (도메인 따라 10~40자)
  - 정규화: 유니코드 NFKC, 공백 압축, 불필요한 개행/제어문자 제거 등
- 방식
  - 문장 단위 청킹
    - 의미 단위가 잘 보존되어 문맥 손실이 적음
    - 문장 길이가 들쭉날쭉한 경우 유사도 매칭이 불안정해짐
  - 토큰 단위 청킹
    - 각 chunk의 길이가 균일해 품질이 안정적
    - 한 문장 또는 의미단위가 둘로 쪼개질 수 있어 문맥이 손실될 수 있음
    - overlap을 늘려 보완
  - 의미 단위 청킹
    - 사람이 읽는 단위와 비슷하게 청킹하여 검색 품질이 높음
    - 구조적 정보(리스트/표 등)를 보존하기 쉬움
    - 의미 단위가 커지는 경우 한 차원 더 청킹해야 함
  - 다이나믹 청킹
    - 텍스트의 성격에 따라 chunk 길이를 가변적으로 결정
    - 효율과 검색 성능이 높음
    - 문서 구조, 자연어의 의미 단위 감지 등 전처리가 필요하고 로직이 복잡해 구현이 어려움



## FAISS

- Facebook AI가 만든 고성능 벡터 검색 라이브러리

- CPU/GPU 버전 지원

- 인덱스 종류

  - 정확도-속도-메모리 radeoff 고려하여 선택

  - `IndexFlatIP/L2` : 정확한 검색. 소~중규모에서 간단하게 사용할 수 있고 안정적으로 작동

  - `IVF` : 클러스터 기반 근사 검색. 검색 속도↑ 메모리↓ 정확도↓

  - `PQ`/`OPQ` : 양자화로 메모리 극저감. Recall 손실 감수. `IVF-PQ` 를 함께 사용하는 조합이 자주 사용됨

  - `HNSW` : 그래프 기반 근사 검색. 검색 속도↑ 메모리↑



### 사용법

- 저장 :  `faiss.write_index()`
- 로드 :  `faiss.read_index()`

