 # k8s의 탄력적 확장(Elastic Scaling)



## 개요

- VM 내에서 scale-out을 할 수 있는 유휴 리소스를 남겨 두고 컨테이너를 운영하는 것 자체가 좀 비효율적인 것 같다는 생각에서 출발
- 내가 멘토링하고 있는 팀에서 공유하고 있는 문서에 컨테이너 단의 autoscaling은 기술되어 있는데 VM 단의 autoscaling에 대한 내용은 없어서, 작성자에게 관련 내용 설명해주면서 나도 공부했음
- k8s가 두 레벨에서의 auto-scaling을 어떻게 실행하는지, 개발자는 어떻게 그 흐름을 설계해야 하는지 정리



## 작동 원리

- HPA와 CA를 중심으로 한 구성요소들이 상호 보완적으로 작동하며 트래픽 변화에 대응

- HPA(Horizontal Pod Autoscaler)
  - pod의 개수를 조절하여 순간적인 트래픽 급증에 빠르게 대응
  - Target Utilization을 기준으로 작동
  - 트래픽이 급증하면 requests와 limits 사이의 headroom 공간까지 사용하며 pod 개수를 일시적으로 늘렸다가, CA가 새 node를 띄워 서빙하기 시작하면 다시 target 수준으로 pod을 줄여나감
- CA(Cluster Autoscaler)
  - 지속적인 부하 증가에 대응하여 전체 cluster의 근본적인 처리 용량을 증설 / 반대로도 작동
  - VM을 새로 띄워서 환경을 구성해야 하기 때문에 상대적으로 느림
  - scale-out
    - 리소스 부족으로 인해 pending 상태인 Pod이 발생하면
    - 클라우드 제공자의 API를 호출하여 새 VM 생성 후 클러스터에 추가
  - scale-in
    - node 내 pod의 requests 총합이 전체 용량의 일정 비율 이하인 상태가 지속되고
    - 그 node의 모든 pod을 다른 node들로 분산하더라도 안정성이 유지되는지 시뮬레이션
    - 두 조건을 모두 확인하고 괜찮으면 pod 이사시킨 후 node 종료



## 질문 / 답변

- Q1. VM이 추가되면서 거기서 추가 컨테이너가 돌기 시작하면 임시로 scale-out되었던 기존 VM들 안에 있는 컨테이너의 일부가 꺼지기도 하겠지?

  - 물론 그렇지. 클러스터 전체적으로 리소스 여유가 생기면 일시적으로 늘어난 pod들은 다시 없애줌

- Q2. 그러려면 limits와는 별개로 리소스의 적정 수준에 대한 변수가 따로 있거나, 사전적인 상태 정의가 필요할 것 같은데.

  - 그치. 그 적정 수준에 대한 선언을 바로 HPA에 해두는 거.
  - 일시적으로 node 내에서 scale-out을 했다가 node가 늘어나면 다시 scale-in을 하는 과정이 하나의 파이프라인 안에 담긴게 아니고
  - 선언해둔 상태와 현 상태에 대한 신호들을 트리거로 하여 각 서비스(스케쥴러, HPA, CA...)들이 기능을 실행하는 유기적인 구조를 이루고 있음

  

## 소감

- 각 기능이 아주 정밀하게 모듈화되어 있다. 아름답다.

- 이전까지는 교재와 docs 기반으로 큰 개념만 훑어왔는데
  - pod 개수를 scale-out하는 방식의 한계점을 스스로 떠올려서 공부를 시작하고
  - 설계 철학과 전체적인 구조를 기반으로, 있을 법한 기능이나 변수, 상태값을 역으로 추적해가며 구조를 파악했음
  - 모처럼 번뜩이는 감각이 살아난다. 학습 범위도 더 넓혀나가고 싶음

